{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e675b9c3-03b7-4433-a5a0-abd2c0cec719",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac0fc1f-f6ca-4d6f-8858-628687ce268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import gc\n",
    "\n",
    "import cupy\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from mmcv.utils import Config, DictAction\n",
    "except:\n",
    "    from mmengine import Config, DictAction\n",
    "from mono.utils.logger import setup_logger\n",
    "import glob\n",
    "from mono.utils.comm import init_env\n",
    "from mono.model.monodepth_model import get_configured_monodepth_model\n",
    "from mono.utils.running import load_ckpt\n",
    "from mono.utils.do_test import transform_test_data_scalecano, get_prediction\n",
    "from mono.utils.custom_data import load_from_annos, load_data\n",
    "\n",
    "from mono.utils.avg_meter import MetricAverageMeter\n",
    "from mono.utils.visualization import save_val_imgs, create_html, save_raw_imgs, save_normal_val_imgs\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image, ExifTags\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mono.utils.unproj_pcd import reconstruct_pcd, save_point_cloud, ply_to_obj\n",
    "from mono.utils.transform import gray_to_colormap\n",
    "from mono.utils.visualization import vis_surface_normal\n",
    "import gradio as gr\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e01ae-672d-4586-b2f1-e529aaaab49a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01920e11-abed-48d9-9662-f850e125f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_depth_normal(img, model_selection=\"vit-large\", fx=1000.0, fy=1000.0, state_cache={}):\n",
    "    cfg_large = Config.fromfile('/home/hydra/workspace/tamp_warm_start/notebooks/metric3d/mono/configs/HourglassDecoder/vit.raft5.large.py')\n",
    "    model_large = get_configured_monodepth_model(cfg_large, )\n",
    "    model_large, _,  _, _ = load_ckpt('/home/hydra/workspace/tamp_warm_start/notebooks/metric3d/weight/metric_depth_vit_large_800k.pth', model_large, strict_match=False)\n",
    "    model_large.eval()\n",
    "    \n",
    "    cfg_small = Config.fromfile('/home/hydra/workspace/tamp_warm_start/notebooks/metric3d/mono/configs/HourglassDecoder/vit.raft5.small.py')\n",
    "    model_small = get_configured_monodepth_model(cfg_small, )\n",
    "    model_small, _,  _, _ = load_ckpt('/home/hydra/workspace/tamp_warm_start/notebooks/metric3d/weight/metric_depth_vit_small_800k.pth', model_small, strict_match=False)\n",
    "    model_small.eval()\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    model_large.to(device)\n",
    "    model_small.to(device)\n",
    "\n",
    "    if model_selection == \"vit-small\":\n",
    "        model = model_small\n",
    "        cfg = cfg_small\n",
    "    elif model_selection == \"vit-large\":\n",
    "        model = model_large\n",
    "        cfg = cfg_large\n",
    "    else:\n",
    "        return None, None, None, None, state_cache, \"Not implemented model.\"\n",
    "    \n",
    "    if img is None:\n",
    "        return None, None, None, None, state_cache, \"Please upload an image and wait for the upload to complete.\"\n",
    "\n",
    "    cv_image = np.array(img) \n",
    "    img = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n",
    "    intrinsic = [fx, fy, img.shape[1]/2, img.shape[0]/2]\n",
    "    rgb_input, cam_models_stacks, pad, label_scale_factor = transform_test_data_scalecano(img, intrinsic, cfg.data_basic)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_depth, pred_depth_scale, scale, output, confidence = get_prediction(\n",
    "                    model = model,\n",
    "                    input = rgb_input,\n",
    "                    cam_model = cam_models_stacks,\n",
    "                    pad_info = pad,\n",
    "                    scale_info = label_scale_factor,\n",
    "                    gt_depth = None,\n",
    "                    normalize_scale = cfg.data_basic.depth_range[1],\n",
    "                    ori_shape=[img.shape[0], img.shape[1]],\n",
    "                )\n",
    "\n",
    "        pred_normal = output['normal_out_list'][0][:, :3, :, :] \n",
    "        H, W = pred_normal.shape[2:]\n",
    "        pred_normal = pred_normal[:, :, pad[0]:H-pad[1], pad[2]:W-pad[3]]\n",
    "\n",
    "    pred_depth = pred_depth.squeeze().cpu().numpy()\n",
    "    pred_depth[pred_depth<0] = 0\n",
    "    pred_color = gray_to_colormap(pred_depth)\n",
    "\n",
    "    pred_normal = torch.nn.functional.interpolate(pred_normal, [img.shape[0], img.shape[1]], mode='bilinear').squeeze()\n",
    "    pred_normal = pred_normal.permute(1,2,0)\n",
    "    pred_color_normal = vis_surface_normal(pred_normal)\n",
    "    pred_normal = pred_normal.cpu().numpy()\n",
    "    \n",
    "    # Storing depth and normal map in state for potential 3D reconstruction\n",
    "    state_cache['depth'] = pred_depth\n",
    "    state_cache['normal'] = pred_normal\n",
    "    state_cache['img'] = img\n",
    "    state_cache['intrinsic'] = intrinsic\n",
    "    state_cache['confidence'] = confidence \n",
    "\n",
    "    # save depth and normal map to .npy file\n",
    "    if 'save_dir' not in state_cache:\n",
    "        cache_id = np.random.randint(0, 100000000000)\n",
    "        while osp.exists(f'recon_cache/{cache_id:08d}'):\n",
    "            cache_id = np.random.randint(0, 100000000000)\n",
    "        state_cache['save_dir'] = f'recon_cache/{cache_id:08d}'\n",
    "        os.makedirs(state_cache['save_dir'], exist_ok=True)\n",
    "    depth_file = f\"{state_cache['save_dir']}/depth.npy\"\n",
    "    normal_file = f\"{state_cache['save_dir']}/normal.npy\"\n",
    "    np.save(depth_file, pred_depth)\n",
    "    np.save(normal_file, pred_normal)\n",
    "\n",
    "    ##formatted = (output * 255 / np.max(output)).astype('uint8')\n",
    "    img = Image.fromarray(pred_color)\n",
    "    img_normal = Image.fromarray(pred_color_normal)\n",
    "\n",
    "    del model_large, model_small\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return img, depth_file, img_normal, normal_file, state_cache, \"Success!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c044a99-5a33-4a7e-9f05-64650124f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprojection_pcd(state_cache, name):\n",
    "    depth_map = state_cache.get('depth', None)\n",
    "    normal_map = state_cache.get('normal', None)\n",
    "    img = state_cache.get('img', None)\n",
    "    intrinsic = state_cache.get('intrinsic', None)\n",
    "\n",
    "    if depth_map is None or img is None:\n",
    "        return None, \"Please predict depth and normal first.\"\n",
    "    \n",
    "    # # downsample/upsample the depth map to confidence map size\n",
    "    # confidence = state_cache.get('confidence', None)\n",
    "    # if confidence is not None:\n",
    "    #     H, W = confidence.shape\n",
    "    #     # intrinsic[0] *= W / depth_map.shape[1]\n",
    "    #     # intrinsic[1] *= H / depth_map.shape[0]\n",
    "    #     # intrinsic[2] *= W / depth_map.shape[1]\n",
    "    #     # intrinsic[3] *= H / depth_map.shape[0]\n",
    "    #     depth_map = cv2.resize(depth_map, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "    #     img = cv2.resize(img, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    #     # filter out depth map by confidence\n",
    "    #     mask = confidence.cpu().numpy() > 0\n",
    "\n",
    "    # downsample the depth map if too large\n",
    "    if depth_map.shape[0] > 1080:\n",
    "        scale = 1080 / depth_map.shape[0]\n",
    "        depth_map = cv2.resize(depth_map, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "        intrinsic = [intrinsic[0]*scale, intrinsic[1]*scale, intrinsic[2]*scale, intrinsic[3]*scale]\n",
    "    \n",
    "    if 'save_dir' not in state_cache:\n",
    "        cache_id = np.random.randint(0, 100000000000)\n",
    "        while osp.exists(f'recon_cache/{cache_id:08d}'):\n",
    "            cache_id = np.random.randint(0, 100000000000)\n",
    "        state_cache['save_dir'] = f'recon_cache/{cache_id:08d}'\n",
    "        os.makedirs(state_cache['save_dir'], exist_ok=True)\n",
    "\n",
    "    pcd_ply = f\"{state_cache['save_dir']}/output_{name}.ply\"\n",
    "    pcd_obj = pcd_ply.replace(\".ply\", \".obj\")\n",
    "\n",
    "    pcd = reconstruct_pcd(depth_map, intrinsic[0], intrinsic[1], intrinsic[2], intrinsic[3])\n",
    "    # if mask is not None:\n",
    "    #     pcd_filtered = pcd[mask]\n",
    "    #     img_filtered = img[mask]\n",
    "    pcd_filtered = pcd.reshape(-1, 3)\n",
    "    img_filtered = img.reshape(-1, 3)\n",
    "\n",
    "    save_point_cloud(pcd_filtered, img_filtered, pcd_ply, binary=False)\n",
    "    # ply_to_obj(pcd_ply, pcd_obj)\n",
    "\n",
    "    # downsample the point cloud for visualization\n",
    "    num_samples = 250000\n",
    "    if pcd_filtered.shape[0] > num_samples:\n",
    "        indices = np.random.choice(pcd_filtered.shape[0], num_samples, replace=False)\n",
    "        pcd_downsampled = pcd_filtered[indices]\n",
    "        img_downsampled = img_filtered[indices]\n",
    "    else:\n",
    "        pcd_downsampled = pcd_filtered\n",
    "        img_downsampled = img_filtered\n",
    "\n",
    "    # plotly show\n",
    "    color_str = np.array([f\"rgb({r},{g},{b})\" for b,g,r in img_downsampled])\n",
    "    data=[go.Scatter3d(\n",
    "        x=pcd_downsampled[:,0],\n",
    "        y=pcd_downsampled[:,1],\n",
    "        z=pcd_downsampled[:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=color_str,\n",
    "            opacity=0.8,\n",
    "        )\n",
    "    )]\n",
    "    layout = go.Layout(\n",
    "        margin=dict(l=0, r=0, b=0, t=0),\n",
    "        scene=dict(\n",
    "            camera = dict(\n",
    "                eye=dict(x=0, y=0, z=-1),\n",
    "                up=dict(x=0, y=-1, z=0)\n",
    "            ),\n",
    "            xaxis=dict(showgrid=False, showticklabels=False, visible=False),\n",
    "            yaxis=dict(showgrid=False, showticklabels=False, visible=False),\n",
    "            zaxis=dict(showgrid=False, showticklabels=False, visible=False),\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return fig, pcd_ply, \"Success!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5cbd9e-ec80-4a0b-928a-952f4470a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pointcloud(img, name):    \n",
    "    depth_output, depth_output_scale, normal_output, normal_file, state_cache, message_box = predict_depth_normal(img)\n",
    "    pcd_output, pcd_ply, message_box = unprojection_pcd(state_cache, name)\n",
    "    return state_cache.get('depth', None), pcd_output, pcd_ply, depth_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e9275-7e78-407a-aa17-a5ced8073205",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71c164ed-6419-4101-aaea-7b9398a752f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\"../flux/generated_images/grasp/floor/image0.png\")\n",
    "# depth_output_match, pcd_output_match, pcd_ply_match = generate_pointcloud(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1380f594-6634-44e6-98dc-4eb9ca9cff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_output_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9813f5c2-9e42-49a3-9450-e8648fb5ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd_output_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "471a630b-ff7a-4e6f-9fea-6bd83503ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open(\"/home/ilyass/Pictures/Screenshots/reference.png\")\n",
    "# pcd_output_ref, pcd_ply_ref = generate_pointcloud(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bb026a1-bc6e-425b-b0ab-225fd30e44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd_output_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e7993-990e-44b8-8305-ddbb02280653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
