{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bdb825-237a-4bd0-a323-8b0131452b45",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3949b619-80ab-487f-ad91-1f26117e5e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 12:57:28.744733: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-11 12:57:28.754279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-11 12:57:28.766457: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-11 12:57:28.769991: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-11 12:57:28.778765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 12:57:29.445257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-11 12:57:31,616] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilyass/mambaforge/envs/tamp/lib/python3.10/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import tempfile\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from torchvision import transforms\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575412d-fda1-4a61-b592-a78b1859f1bc",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45bf618-8811-46b1-875d-fa02cdd039c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    SAM2 Plotting \n",
    "\"\"\"\n",
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4910181d-398c-4c15-bdd2-f4cac8401653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Florence VLM\n",
    "\"\"\"\n",
    "def run_example(task_prompt, image, processor, model, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to('cuda', torch.float16)\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"].cuda(),\n",
    "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n",
    "\n",
    "def convert_to_od_format(data):  \n",
    "    \"\"\"  \n",
    "    Converts a dictionary with 'bboxes' and 'bboxes_labels' into a dictionary with separate 'bboxes' and 'labels' keys.  \n",
    "  \n",
    "    Parameters:  \n",
    "    - data: The input dictionary with 'bboxes', 'bboxes_labels', 'polygons', and 'polygons_labels' keys.  \n",
    "  \n",
    "    Returns:  \n",
    "    - A dictionary with 'bboxes' and 'labels' keys formatted for object detection results.  \n",
    "    \"\"\"  \n",
    "    # Extract bounding boxes and labels  \n",
    "    bboxes = data.get('bboxes', [])  \n",
    "    labels = data.get('bboxes_labels', [])  \n",
    "      \n",
    "    # Construct the output format  \n",
    "    od_results = {  \n",
    "        'bboxes': bboxes,  \n",
    "        'labels': labels  \n",
    "    }  \n",
    "    \n",
    "    return od_results  \n",
    "\n",
    "def plot_bbox(image, data):\n",
    "   # Create a figure and axes  \n",
    "    fig, ax = plt.subplots()  \n",
    "      \n",
    "    # Display the image  \n",
    "    ax.imshow(image)  \n",
    "      \n",
    "    # Plot each bounding box  \n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):  \n",
    "        # Unpack the bounding box coordinates  \n",
    "        x1, y1, x2, y2 = bbox  \n",
    "        # Create a Rectangle patch  \n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')  \n",
    "        # Add the rectangle to the Axes  \n",
    "        ax.add_patch(rect)  \n",
    "        # Annotate the label  \n",
    "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))  \n",
    "      \n",
    "    # Remove the axis ticks and labels  \n",
    "    ax.axis('off')  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c430e-2c68-47ed-bf9d-f9d5e86ea2be",
   "metadata": {},
   "source": [
    "# Device Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864922e1-7e2c-491e-a6d2-45e49ad798d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b94e7e-2b25-40e5-a627-436bf5c15a0a",
   "metadata": {},
   "source": [
    "# Sapiens Foundation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1df6d3-95ce-4645-8b64-175258dc0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu():\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95bcde1-5f94-47a2-9af3-ac9e9d85ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sapiens_segmentation(img):\n",
    "    image_processor = ImageProcessor()\n",
    "    segmentation_image, npy_path = image_processor.process_image_segmentation(img, \"1bs\")\n",
    "    clean_gpu()\n",
    "    return segmentation_image, npy_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cc63a-e1fd-40a8-b62d-25fefad721ce",
   "metadata": {},
   "source": [
    "#### Mask Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5917b931-ee92-47ac-b1b8-7ddef42cfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def florence_sam2_segmentation(img, text_input):\n",
    "    # Load Florence model\n",
    "    model_id = 'microsoft/Florence-2-large'\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().to(\"cuda\")\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # Load SAM2 model\n",
    "    predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "\n",
    "    task_prompt = '<OPEN_VOCABULARY_DETECTION>'\n",
    "    results = run_example(task_prompt, img, processor, model, text_input)\n",
    "    \n",
    "    bbox_results  = convert_to_od_format(results['<OPEN_VOCABULARY_DETECTION>'])\n",
    "    \n",
    "    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        \n",
    "        predictor.set_image(img)\n",
    "    \n",
    "        input_point_x = (bbox_results['bboxes'][0][0] + bbox_results['bboxes'][0][2])/2\n",
    "        input_point_y = (bbox_results['bboxes'][0][1] + bbox_results['bboxes'][0][3])/2\n",
    "        input_point = np.array([[input_point_x, input_point_y]])\n",
    "        input_label = np.array([1])\n",
    "    \n",
    "        masks, scores, logits = predictor.predict(box=bbox_results['bboxes'][0],\n",
    "                                                  multimask_output=False)\n",
    "\n",
    "    clean_gpu()\n",
    "    del model, processor, predictor\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return masks, scores, logits, bbox_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
