{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bdb825-237a-4bd0-a323-8b0131452b45",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3949b619-80ab-487f-ad91-1f26117e5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import tempfile\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from transformers import pipeline\n",
    "from torchvision import transforms\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM \n",
    "from detector_utils import adapt_mmdet_pipeline, init_detector, process_images_detector\n",
    "from classes_and_palettes import (\n",
    "    COCO_KPTS_COLORS,\n",
    "    COCO_WHOLEBODY_KPTS_COLORS,\n",
    "    GOLIATH_KPTS_COLORS,\n",
    "    GOLIATH_SKELETON_INFO,\n",
    "    GOLIATH_KEYPOINTS,\n",
    "    GOLIATH_PALETTE, \n",
    "    GOLIATH_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95123dd8-1422-48f5-8b02-f6f0d4c880ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575412d-fda1-4a61-b592-a78b1859f1bc",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45bf618-8811-46b1-875d-fa02cdd039c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    SAM2 Plotting \n",
    "\"\"\"\n",
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "\"\"\"\n",
    "    Sapiense Foundation Models\n",
    "\"\"\"\n",
    "class ConfigSapiens:\n",
    "    ASSETS_DIR = \"/home/ilyass/workspace/tamp_warm_start/notebooks/sapiens/assets/\"\n",
    "    CHECKPOINTS_DIR = \"/home/ilyass/workspace/tamp_warm_start/notebooks/sapiens/assets/checkpoints/\"\n",
    "    CHECKPOINTS = {\n",
    "        \"0.3bp\": \"pose/sapiens_0.3b_goliath_best_goliath_AP_575_torchscript.pt2\",\n",
    "        \"1bp\": \"pose/sapiens_1b_goliath_best_goliath_AP_640_torchscript.pt2\",\n",
    "\n",
    "        \"0.3s\": \"segmentation/sapiens_0.3b_goliath_best_goliath_mIoU_7673_epoch_194_torchscript.pt2\",\n",
    "        \"0.6s\": \"segmentation/sapiens_0.6b_goliath_best_goliath_mIoU_7777_epoch_178_torchscript.pt2\",\n",
    "        \"1bs\": \"segmentation/sapiens_1b_goliath_best_goliath_mIoU_7994_epoch_151_torchscript.pt2\"\n",
    "    }\n",
    "    DETECTION_CHECKPOINT = \"/home/ilyass/workspace/tamp_warm_start/notebooks/sapiens/assets/checkpoints/pose/rtmdet_m_8xb32-100e_coco-obj365-person-235e8209.pth\"\n",
    "    DETECTION_CONFIG = \"/home/ilyass/workspace/tamp_warm_start/notebooks/sapiens/assets/assets_rtmdet_m_640-8xb32_coco-person_no_nms.py\"\n",
    "    \n",
    "class ModelManager:\n",
    "    @staticmethod\n",
    "    def load_model(checkpoint_name: str):\n",
    "        if checkpoint_name is None:\n",
    "            return None\n",
    "\n",
    "        checkpoint_path = os.path.join(ConfigSapiens.CHECKPOINTS_DIR, checkpoint_name)\n",
    "        model = torch.jit.load(checkpoint_path)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.inference_mode()\n",
    "    def run_model_keypoints(model, input_tensor):\n",
    "        return model(input_tensor)\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.inference_mode()\n",
    "    def run_model_segmentation(model, input_tensor, height, width):\n",
    "        output = model(input_tensor)\n",
    "        output = torch.nn.functional.interpolate(output, size=(height, width), mode=\"bilinear\", align_corners=False)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        return preds\n",
    "\n",
    "class ImageProcessor:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((1024, 768)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[123.5/255, 116.5/255, 103.5/255], \n",
    "                                 std=[58.5/255, 57.0/255, 57.5/255])\n",
    "        ])\n",
    "        self.detector = init_detector(\n",
    "            ConfigSapiens.DETECTION_CONFIG, ConfigSapiens.DETECTION_CHECKPOINT, device='cpu'\n",
    "        )\n",
    "        self.detector.cfg = adapt_mmdet_pipeline(self.detector.cfg)\n",
    "\n",
    "    def detect_persons(self, image: Image.Image):\n",
    "        # Convert PIL Image to tensor\n",
    "        image = np.array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # Perform person detection\n",
    "        bboxes_batch = process_images_detector(\n",
    "            image, \n",
    "            self.detector\n",
    "        )\n",
    "        bboxes = self.get_person_bboxes(bboxes_batch[0])  # Get bboxes for the first (and only) image\n",
    "        \n",
    "        return bboxes\n",
    "    \n",
    "    def get_person_bboxes(self, bboxes_batch, score_thr=0.3):\n",
    "        person_bboxes = []\n",
    "        for bbox in bboxes_batch:\n",
    "            if len(bbox) == 5:  # [x1, y1, x2, y2, score]\n",
    "                if bbox[4] > score_thr:\n",
    "                    person_bboxes.append(bbox)\n",
    "            elif len(bbox) == 4:  # [x1, y1, x2, y2]\n",
    "                person_bboxes.append(bbox + [1.0])  # Add a default score of 1.0\n",
    "        return person_bboxes\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def estimate_pose(self, image: Image.Image, bboxes: List[List[float]], model_name: str, kpt_threshold: float):\n",
    "        pose_model = ModelManager.load_model(ConfigSapiens.CHECKPOINTS[model_name])\n",
    "        \n",
    "        result_image = image.copy()\n",
    "        all_keypoints = []  # List to store keypoints for all persons\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            cropped_img = self.crop_image(result_image, bbox)\n",
    "            input_tensor = self.transform(cropped_img).unsqueeze(0).to(device)\n",
    "            heatmaps = ModelManager.run_model_keypoints(pose_model, input_tensor)\n",
    "            keypoints = self.heatmaps_to_keypoints(heatmaps[0].cpu().numpy())\n",
    "            all_keypoints.append(keypoints)  # Collect keypoints\n",
    "            result_image = self.draw_keypoints(result_image, keypoints, bbox, kpt_threshold)\n",
    "        \n",
    "        return result_image, all_keypoints\n",
    "\n",
    "    def process_image_keypoints(self, image: Image.Image, model_name: str, kpt_threshold: str):\n",
    "        bboxes = self.detect_persons(image)\n",
    "        result_image, keypoints = self.estimate_pose(image, bboxes, model_name, float(kpt_threshold))\n",
    "        return result_image, keypoints\n",
    "\n",
    "    def process_image_segmentation(self, image: Image.Image, model_name: str):\n",
    "        model = ModelManager.load_model(ConfigSapiens.CHECKPOINTS[model_name])\n",
    "        input_tensor = self.transform(image).unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        preds = ModelManager.run_model_segmentation(model, input_tensor, image.height, image.width)\n",
    "        mask = preds.squeeze(0).cpu().numpy()\n",
    "\n",
    "        # Visualize the segmentation\n",
    "        blended_image = self.visualize_pred_with_overlay(image, mask)\n",
    "\n",
    "        # Create downloadable .npy file\n",
    "        npy_path = tempfile.mktemp(suffix='.npy')\n",
    "        np.save(npy_path, mask)\n",
    "\n",
    "        return blended_image, npy_path\n",
    "\n",
    "    def crop_image(self, image, bbox):\n",
    "        if len(bbox) == 4:\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "        elif len(bbox) >= 5:\n",
    "            x1, y1, x2, y2, _ = map(int, bbox[:5])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected bbox format: {bbox}\")\n",
    "        \n",
    "        crop = image.crop((x1, y1, x2, y2))\n",
    "        return crop\n",
    "\n",
    "    @staticmethod\n",
    "    def heatmaps_to_keypoints(heatmaps):\n",
    "        num_joints = heatmaps.shape[0]  # Should be 308\n",
    "        keypoints = {}\n",
    "        for i, name in enumerate(GOLIATH_KEYPOINTS):\n",
    "            if i < num_joints:\n",
    "                heatmap = heatmaps[i]\n",
    "                y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
    "                conf = heatmap[y, x]\n",
    "                keypoints[name] = (float(x), float(y), float(conf))\n",
    "        return keypoints\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_keypoints(image, keypoints, bbox, kpt_threshold):\n",
    "        image = np.array(image)\n",
    "\n",
    "        # Handle both 4 and 5-element bounding boxes\n",
    "        if len(bbox) == 4:\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "        elif len(bbox) >= 5:\n",
    "            x1, y1, x2, y2, _ = map(int, bbox[:5])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected bbox format: {bbox}\")\n",
    "                \n",
    "        # Calculate adaptive radius and thickness based on bounding box size\n",
    "        bbox_width = x2 - x1\n",
    "        bbox_height = y2 - y1\n",
    "        bbox_size = np.sqrt(bbox_width * bbox_height)\n",
    "        \n",
    "        radius = max(1, int(bbox_size * 0.006))  # minimum 1 pixel\n",
    "        thickness = max(1, int(bbox_size * 0.006))  # minimum 1 pixel\n",
    "        bbox_thickness = max(1, thickness//4)\n",
    "\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), bbox_thickness)\n",
    "        \n",
    "        # Draw keypoints\n",
    "        for i, (name, (x, y, conf)) in enumerate(keypoints.items()):\n",
    "            if conf > kpt_threshold and i < len(GOLIATH_KPTS_COLORS):\n",
    "                x_coord = int(x * bbox_width / 192) + x1\n",
    "                y_coord = int(y * bbox_height / 256) + y1\n",
    "                color = GOLIATH_KPTS_COLORS[i]\n",
    "                cv2.circle(image, (x_coord, y_coord), radius, color, -1)\n",
    "\n",
    "        # Draw skeleton\n",
    "        for _, link_info in GOLIATH_SKELETON_INFO.items():\n",
    "            pt1_name, pt2_name = link_info['link']\n",
    "            color = link_info['color']\n",
    "            \n",
    "            if pt1_name in keypoints and pt2_name in keypoints:\n",
    "                pt1 = keypoints[pt1_name]\n",
    "                pt2 = keypoints[pt2_name]\n",
    "                if pt1[2] > kpt_threshold and pt2[2] > kpt_threshold:\n",
    "                    x1_coord = int(pt1[0] * bbox_width / 192) + x1\n",
    "                    y1_coord = int(pt1[1] * bbox_height / 256) + y1\n",
    "                    x2_coord = int(pt2[0] * bbox_width / 192) + x1\n",
    "                    y2_coord = int(pt2[1] * bbox_height / 256) + y1\n",
    "                    cv2.line(image, (x1_coord, y1_coord), (x2_coord, y2_coord), color, thickness=thickness)\n",
    "\n",
    "        return Image.fromarray(image)\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_pred_with_overlay(img, sem_seg, alpha=0.5):\n",
    "        img_np = np.array(img.convert(\"RGB\"))\n",
    "        sem_seg = np.array(sem_seg)\n",
    "\n",
    "        num_classes = len(GOLIATH_CLASSES)\n",
    "        ids = np.unique(sem_seg)[::-1]\n",
    "        legal_indices = ids < num_classes\n",
    "        ids = ids[legal_indices]\n",
    "        labels = np.array(ids, dtype=np.int64)\n",
    "\n",
    "        colors = [GOLIATH_PALETTE[label] for label in labels]\n",
    "\n",
    "        overlay = np.zeros((*sem_seg.shape, 3), dtype=np.uint8)\n",
    "\n",
    "        for label, color in zip(labels, colors):\n",
    "            overlay[sem_seg == label, :] = color\n",
    "\n",
    "        blended = np.uint8(img_np * (1 - alpha) + overlay * alpha)\n",
    "        return Image.fromarray(blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4910181d-398c-4c15-bdd2-f4cac8401653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Florence VLM\n",
    "\"\"\"\n",
    "def run_example(task_prompt, image, processor, model, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to('cuda', torch.float16)\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"].cuda(),\n",
    "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n",
    "\n",
    "def convert_to_od_format(data):  \n",
    "    \"\"\"  \n",
    "    Converts a dictionary with 'bboxes' and 'bboxes_labels' into a dictionary with separate 'bboxes' and 'labels' keys.  \n",
    "  \n",
    "    Parameters:  \n",
    "    - data: The input dictionary with 'bboxes', 'bboxes_labels', 'polygons', and 'polygons_labels' keys.  \n",
    "  \n",
    "    Returns:  \n",
    "    - A dictionary with 'bboxes' and 'labels' keys formatted for object detection results.  \n",
    "    \"\"\"  \n",
    "    # Extract bounding boxes and labels  \n",
    "    bboxes = data.get('bboxes', [])  \n",
    "    labels = data.get('bboxes_labels', [])  \n",
    "      \n",
    "    # Construct the output format  \n",
    "    od_results = {  \n",
    "        'bboxes': bboxes,  \n",
    "        'labels': labels  \n",
    "    }  \n",
    "      \n",
    "    return od_results  \n",
    "\n",
    "def plot_bbox(image, data):\n",
    "   # Create a figure and axes  \n",
    "    fig, ax = plt.subplots()  \n",
    "      \n",
    "    # Display the image  \n",
    "    ax.imshow(image)  \n",
    "      \n",
    "    # Plot each bounding box  \n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):  \n",
    "        # Unpack the bounding box coordinates  \n",
    "        x1, y1, x2, y2 = bbox  \n",
    "        # Create a Rectangle patch  \n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')  \n",
    "        # Add the rectangle to the Axes  \n",
    "        ax.add_patch(rect)  \n",
    "        # Annotate the label  \n",
    "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))  \n",
    "      \n",
    "    # Remove the axis ticks and labels  \n",
    "    ax.axis('off')  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c430e-2c68-47ed-bf9d-f9d5e86ea2be",
   "metadata": {},
   "source": [
    "# Device Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864922e1-7e2c-491e-a6d2-45e49ad798d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b94e7e-2b25-40e5-a627-436bf5c15a0a",
   "metadata": {},
   "source": [
    "# Sapiens Foundation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1df6d3-95ce-4645-8b64-175258dc0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu():\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e95bcde1-5f94-47a2-9af3-ac9e9d85ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sapiens_segmentation(img):\n",
    "    image_processor = ImageProcessor()\n",
    "    segmentation_image, npy_path = image_processor.process_image_segmentation(img, \"1bs\")\n",
    "    clean_gpu()\n",
    "    return segmentation_image, npy_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f27e28-4fc0-4d23-8d8b-766ec1643a79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c28382-9a8b-4e8f-943d-3632851f9295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Plot RGB and depth side by side\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# # Plot the first image\n",
    "# ax1.imshow(rgb_image)\n",
    "# ax1.set_title('RGB')\n",
    "# ax1.axis('off')  # Hide axes\n",
    "\n",
    "# # Plot the second image\n",
    "# ax2.imshow(segmentation_image)\n",
    "# ax2.set_title('Segmentation')\n",
    "# ax2.axis('off')  # Hide axes\n",
    "\n",
    "# # Adjust the layout and display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ed279-6886-4520-909d-8c22ae02117a",
   "metadata": {},
   "source": [
    "### Depth + Removed Segmented Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7735d26f-0970-49ea-8a5c-e7897a27afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read mask information\n",
    "# mask = np.load(npy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17673ee-dbcc-4b05-86ec-62a01e93fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_mask = np.where(mask > 0, 255, 0)\n",
    "# cv_mask = np.asarray(cv_mask, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0888eb3-ecc3-4fc2-9e65-2d18d098a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_without_person = cv2.inpaint(np.asarray(depth_image), cv_mask, 40, cv2.INPAINT_TELEA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ccac7c7-69d4-441d-8543-1acb785f84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgb_without_person = cv2.inpaint(np.asarray(rgb_image), cv_mask, 40, cv2.INPAINT_TELEA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de576b49-eca9-464b-b599-267259ef0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot RGB and depth side by side\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "# # Plot the first image\n",
    "# ax1.imshow(depth_image)\n",
    "# ax1.set_title('Original Depth Image')\n",
    "# ax1.axis('off')  # Hide axes\n",
    "\n",
    "# # Plot the second image\n",
    "# ax2.imshow(depth_without_person)\n",
    "# ax2.set_title('Depth Image with Person Removal')\n",
    "# ax2.axis('off')  # Hide axes\n",
    "\n",
    "# # Plot the second image\n",
    "# ax3.imshow(rgb_without_person)\n",
    "# ax3.set_title('RGB Image with Person Removal')\n",
    "# ax3.axis('off')  # Hide axes\n",
    "\n",
    "# # Adjust the layout and display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d811c-e098-4d7c-9384-83a71a2112e0",
   "metadata": {},
   "source": [
    "### Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d3a799e-e9ba-40c9-be7c-e31ddcef0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sapiens_pose(img):\n",
    "    image_processor = ImageProcessor()\n",
    "    keypoints_image, keypoints = image_processor.process_image_keypoints(img, \"1bp\", 0.3)\n",
    "    clean_gpu()\n",
    "    return keypoints_image, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d89efb4-fb58-4816-afda-1f5d96c64cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot RGB and depth side by side\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# # Plot the first image\n",
    "# ax1.imshow(rgb_image)\n",
    "# ax1.set_title('RGB')\n",
    "# ax1.axis('off')  # Hide axes\n",
    "\n",
    "# # Plot the second image\n",
    "# ax2.imshow(keypoints_image)\n",
    "# ax2.set_title('Keypoints')\n",
    "# ax2.axis('off')  # Hide axes\n",
    "\n",
    "# # Adjust the layout and display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196dcf3-0ccd-48fe-a85e-fd87edf02a62",
   "metadata": {},
   "source": [
    "# Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27da8d0-6051-41ab-a5b9-138885bff502",
   "metadata": {},
   "source": [
    "#### Matching Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f18b37-cef3-4a3f-9ddc-34e763ca10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from tensorflow.keras.applications import VGG16\n",
    "# from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import os\n",
    "\n",
    "# # Load pre-trained VGG16 model\n",
    "# model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# def extract_features(img_path):\n",
    "#     img = Image.open(img_path).resize((224, 224))\n",
    "#     img_array = img_to_array(img)\n",
    "#     img_array = img_array[:,:,:3]\n",
    "#     img_array = np.expand_dims(img_array, axis=0)\n",
    "#     img_array = preprocess_input(img_array)\n",
    "#     features = model.predict(img_array)\n",
    "#     return features.flatten()\n",
    "\n",
    "# def find_similar_images(reference_img_path, image_folder, top_n=3):\n",
    "#     # Extract features for reference image\n",
    "#     reference_features = extract_features(reference_img_path)\n",
    "    \n",
    "#     # Extract features for all images in the folder\n",
    "#     image_features = []\n",
    "#     image_paths = []\n",
    "#     for img_name in os.listdir(image_folder):\n",
    "#         if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#             img_path = os.path.join(image_folder, img_name)\n",
    "#             features = extract_features(img_path)\n",
    "#             image_features.append(features)\n",
    "#             image_paths.append(img_path)\n",
    "    \n",
    "#     # Calculate similarities\n",
    "#     similarities = cosine_similarity([reference_features], image_features)[0]\n",
    "    \n",
    "#     # Sort and get top N similar images\n",
    "#     top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "#     top_similar = [(image_paths[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "#     return top_similar\n",
    "\n",
    "# # Usage example\n",
    "# reference_image = '/home/ilyass/reference.png'\n",
    "# # reference_image = './generated_images/grasp/floor/image0.png'\n",
    "# image_folder = './generated_images/grasp/floor'\n",
    "# # image_folder = '/home/ilyass/Pictures/Screenshots'\n",
    "# similar_images = find_similar_images(reference_image, image_folder)\n",
    "\n",
    "# for img_path, similarity in similar_images:\n",
    "#     print(f\"Image: {img_path}, Similarity: {similarity}\")\n",
    "\n",
    "# clean_gpu()\n",
    "\n",
    "# del model\n",
    "# import gc\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cc63a-e1fd-40a8-b62d-25fefad721ce",
   "metadata": {},
   "source": [
    "#### Mask Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5917b931-ee92-47ac-b1b8-7ddef42cfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def florence_sam2_segmentation(img):\n",
    "    # Load Florence model\n",
    "    model_id = 'microsoft/Florence-2-large'\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().to(\"cuda\")\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # Load SAM2 model\n",
    "    predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n",
    "\n",
    "    task_prompt = '<OPEN_VOCABULARY_DETECTION>'\n",
    "    results = run_example(task_prompt, img, processor, model, text_input=\"a box\")\n",
    "\n",
    "    bbox_results  = convert_to_od_format(results['<OPEN_VOCABULARY_DETECTION>'])\n",
    "\n",
    "    with torch.inference_mode(), torch.autocast(\"cpu\", dtype=torch.bfloat16):\n",
    "        \n",
    "        predictor.set_image(img)\n",
    "    \n",
    "        input_point_x = (bbox_results['bboxes'][0][0] + bbox_results['bboxes'][0][2])/2\n",
    "        input_point_y = (bbox_results['bboxes'][0][1] + bbox_results['bboxes'][0][3])/2\n",
    "        input_point = np.array([[input_point_x, input_point_y]])\n",
    "        input_label = np.array([1])\n",
    "    \n",
    "        masks, scores, logits = predictor.predict(point_coords=input_point,\n",
    "                                                  point_labels=input_label,\n",
    "                                                  multimask_output=False)\n",
    "\n",
    "    clean_gpu()\n",
    "    del model, processor, predictor\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return masks, scores, logits, bbox_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f257bc-05f3-4d5c-a950-4d1a25865627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reference Image\n",
    "# input_point = None\n",
    "# input_label = None\n",
    "\n",
    "# ref_reg_image = Image.open(\"/home/ilyass/reference.png\")\n",
    "# masks_ref, scores_ref, logits_ref, bbox_results_ref = extract_mask(ref_reg_image)\n",
    "# show_masks(ref_reg_image, masks_ref, scores_ref, box_coords=bbox_results_ref['bboxes'][0], point_coords=input_point, input_labels=input_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "253402f4-6af8-4f44-87f4-7050c676cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reference Image\n",
    "# input_point = None\n",
    "# input_label = None\n",
    "\n",
    "# match_rgb_image = Image.open(\"./generated_images/grasp/floor/image13.png\")\n",
    "# masks_match, scores_match, logits_match, bbox_results_match = extract_mask(match_rgb_image)\n",
    "# show_masks(match_rgb_image, masks_match, scores_match, box_coords=bbox_results_match['bboxes'][0], point_coords=input_point, input_labels=input_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
